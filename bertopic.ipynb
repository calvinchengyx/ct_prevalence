{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb0ca6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bertopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61037da5",
   "metadata": {},
   "source": [
    "# 1. Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c007986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset calvin_posts contains: 555872 records\n",
      "However, 0 records are missing values (worth checking) - 0.00% missing rate\n",
      "Platform distribution for bertopic is executed for following data:\n",
      "platform\n",
      "truthsocial    186506\n",
      "gab            112722\n",
      "X               86061\n",
      "4chan           62638\n",
      "bluesky         53242\n",
      "gettr           42091\n",
      "fediverse       12612\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "INPUT = \"/VData/scro4316/ct_prevalence/calvin_posts_cleaned.parquet\"\n",
    "df = pd.read_parquet(INPUT)\n",
    "total = df['post_text'].shape[0]\n",
    "na_rows = df['post_text'].isna().sum()\n",
    "missing_rate = na_rows / total * 100\n",
    "platform_counts = df['platform'].value_counts()\n",
    "\n",
    "print(f\"The dataset calvin_posts contains: {total} records\")\n",
    "print(f\"However, {na_rows} records are missing values (worth checking) - {missing_rate:.2f}% missing rate\")\n",
    "print(\"Platform distribution for bertopic is executed for following data:\")\n",
    "print(platform_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b911f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'platform', 'topic', 'post_text', 'post_clean', 'embed_id'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d206a6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platform\n",
      "truthsocial    186506\n",
      "gab            112722\n",
      "X               86061\n",
      "4chan           62638\n",
      "bluesky         53242\n",
      "gettr           42091\n",
      "fediverse       12612\n",
      "Name: count, dtype: int64\n",
      "topic\n",
      "KEYWORDS_COVID19    275457\n",
      "KEYWORDS_NWO        218558\n",
      "KEYWORDS_ALIEN       49343\n",
      "KEYWORDS_9_11         8147\n",
      "KEYWORDS_MOON         4367\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['platform'].value_counts())\n",
    "print(df['topic'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc337c6",
   "metadata": {},
   "source": [
    "# 2. Preprocessing\n",
    "- remove NA content, duplicates, urls, from 'post_text' column.\n",
    "- use index str as `embed id` to avoid matching erros in long-id strings (with letters, numbers and special characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "249b84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# ===== PRE-COMPILE ALL REGEX PATTERNS (ONE TIME ONLY) =====\n",
    "URL_PATTERN = re.compile(r'https?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "PARTIAL_URL_PATTERN = re.compile(r'https?://\\S*')\n",
    "DOMAIN_PATTERN = re.compile(r'\\s(?:www\\.|(?:[\\w-]+\\.)+(?:com|net|org|edu|gov|mil|biz|info|io|me|tv|[\\w]{2,}))\\S*')\n",
    "FRAGMENTS_PATTERN = re.compile(r'(?:press\\.coop|gab\\.com|youtube\\.com|bitchute\\.com|imdb\\.com)\\/\\S*')\n",
    "ASCII_PATTERN = re.compile(r'[^\\x00-\\x7F]+')\n",
    "SPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "def remove_non_english(text):\n",
    "    \"\"\"\n",
    "    Language detection using langdetect\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) < 3:\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        detected_lang = detect(text)\n",
    "        if detected_lang != 'en':\n",
    "            return None\n",
    "        return text\n",
    "    except LangDetectException:\n",
    "        # If detection fails, keep the text\n",
    "        return text    \n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Remove all URLs from the text using pre-compiled patterns\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Use pre-compiled patterns (NO recompilation)\n",
    "    text = URL_PATTERN.sub('', text)\n",
    "    text = PARTIAL_URL_PATTERN.sub('', text)\n",
    "    text = DOMAIN_PATTERN.sub(' ', text)\n",
    "    text = FRAGMENTS_PATTERN.sub('', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function with optimized regex\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return None\n",
    "    \n",
    "    # First check if text is English (before removing content)\n",
    "    text = remove_non_english(text)\n",
    "    if text is None:\n",
    "        return None\n",
    "        \n",
    "    # Then remove URLs\n",
    "    text = remove_urls(text)\n",
    "    \n",
    "    # Remove emojis and non-ASCII characters using pre-compiled pattern\n",
    "    text = ASCII_PATTERN.sub('', text)\n",
    "    \n",
    "    # Remove multiple spaces using pre-compiled pattern\n",
    "    text = SPACE_PATTERN.sub(' ', text).strip()\n",
    "    \n",
    "    # Return None if text becomes empty after cleaning\n",
    "    return text if len(text) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "248915cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 86/555872 [00:00<10:48, 856.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 555872/555872 [14:42<00:00, 630.19it/s] \n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df = df.drop_duplicates(subset=['id'])\n",
    "df['post_clean'] = df['post_text'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53891a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data\n",
    "df = df.reset_index(drop=True)\n",
    "df['embed_id'] = df.index.astype(str)\n",
    "OUTPUT = \"/VData/scro4316/ct_prevalence/calvin_posts_cleaned.parquet\"\n",
    "df.to_parquet(OUTPUT, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e684ee",
   "metadata": {},
   "source": [
    "# 3. pre-embed texts with Mpnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74069fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/VData/scro4316/ct_prevalence/calvin_posts_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "149a4574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device: 1\n",
      "Using device: _CudaDeviceProperties(name='NVIDIA H100 80GB HBM3', major=9, minor=0, total_memory=81105MB, multi_processor_count=132, uuid=1cc4a5f1-9c3a-e533-51f5-2db38a687abb, L2_cache_size=50MB)\n",
      "Final validation: 554943 valid documents out of 555872\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import datetime\n",
    "import numpy as np\n",
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear any existing CUDA memory first\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Set GPU device to 1 (and verify it)\n",
    "torch.cuda.set_device(1)\n",
    "DEVICE = \"cuda:1\"\n",
    "print(\"Current CUDA device:\", torch.cuda.current_device())\n",
    "print(\"Using device:\", torch.cuda.get_device_properties(1))\n",
    "\n",
    "SERVER = \"/VData/scro4316/ct_prevalence\"\n",
    "OUTPUT_PATH = f\"{SERVER}/results\"\n",
    "EMBEDDINGS_OUTPUT_PATH = f\"{SERVER}/embeddings\"\n",
    "\n",
    "#  Embed claim text and save it to the embedding output ######\n",
    "# Prepare data for embedding (using unique texts only)\n",
    "ids = df['embed_id'].astype(str).tolist()  # Representative post_ids for unique texts\n",
    "docs = df['post_clean'].tolist()  # Unique texts only\n",
    "\n",
    "# Additional validation - ensure all docs are strings\n",
    "clean_docs = []\n",
    "clean_ids = []\n",
    "# ensure no 0-length strings after cleaning\n",
    "for i, doc in enumerate(docs):\n",
    "    if isinstance(doc, str) and len(doc.strip()) > 0:\n",
    "        clean_docs.append(doc.strip())\n",
    "        clean_ids.append(ids[i])\n",
    "\n",
    "print(f\"Final validation: {len(clean_docs)} valid documents out of {len(docs)}\")\n",
    "ids = clean_ids\n",
    "docs = clean_docs\n",
    "CHUNK_SIZE = 10000  # Number of documents per chunk\n",
    "\n",
    "#### 3. Generate embeddings and save them in chunks ######\n",
    "def generate_and_save_embeddings(docs, ids, output_path, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"\n",
    "    Generate embeddings for documents using OpenAI API and save them in chunks.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Verify GPU device before loading model\n",
    "    print(f\"Current GPU before model load: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    # Load embedding model\n",
    "    print(\"Loading all-mpnet-base-v2 model...\")\n",
    "    print(\"Before model load:\", torch.cuda.memory_allocated(device=DEVICE) / 1e9, \"GB\")\n",
    "    model = sentence_transformers.SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=DEVICE)\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(\"After model load:\", torch.cuda.memory_allocated(device=DEVICE) / 1e9, \"GB\")\n",
    "    \n",
    "    # Split into chunks\n",
    "    docs_chunks = [docs[x:x+chunk_size] for x in range(0, len(docs), chunk_size)]\n",
    "    ids_chunks = [ids[x:x+chunk_size] for x in range(0, len(ids), chunk_size)]\n",
    "    \n",
    "    for i in range(len(docs_chunks)):\n",
    "        out_file = f\"{output_path}/embeddings_{i+1}.npy\"\n",
    "        if os.path.isfile(out_file):\n",
    "            print(f\"Chunk {i+1} already exists, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing chunk {i+1} of {len(docs_chunks)}\")\n",
    "        print(f\"Starting at {datetime.datetime.now()}\")\n",
    "        print(f\"Current GPU device: {torch.cuda.current_device()}\")  # Verify GPU for each chunk\n",
    "        \n",
    "        # Generate embeddings for this chunk\n",
    "        embeddings = model.encode(docs_chunks[i], show_progress_bar=True, batch_size=32, device=DEVICE)\n",
    "        embeddings_dict = dict(zip(ids_chunks[i], embeddings))\n",
    "        \n",
    "        # Save chunk to file\n",
    "        np.save(out_file, embeddings_dict)\n",
    "        print(f\"Saved {len(embeddings_dict)} embeddings to {out_file}\")\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del embeddings\n",
    "        del embeddings_dict\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        print('Memory after cleanup:', round(torch.cuda.memory_allocated(1)/1024**3,1), 'GB')\n",
    "    \n",
    "    print(\"Embedding generation completed!\")\n",
    "\n",
    "def load_embeddings(file_path, ids_order=None):\n",
    "    \"\"\"Load embeddings from a numpy file.\"\"\"\n",
    "    try:\n",
    "        embeddings_dict = np.load(file_path, allow_pickle=True).item()\n",
    "        if ids_order:\n",
    "            embeddings_ordered = {id: embeddings_dict[id] for id in ids_order if id in embeddings_dict}\n",
    "            return embeddings_ordered\n",
    "        return embeddings_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load embeddings from {file_path} with error {e}\")\n",
    "        return None\n",
    "\n",
    "def load_all_embeddings(folder_path, ids_order=None):\n",
    "    \"\"\"Load all embeddings from a directory.\"\"\"\n",
    "    embeddings_dict = {}\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Embeddings folder {folder_path} does not exist!\")\n",
    "        return embeddings_dict\n",
    "    \n",
    "    # Get filenames and sort by number\n",
    "    filenames = sorted(os.listdir(folder_path), key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "    \n",
    "    for file_name in filenames:\n",
    "        if file_name.endswith('.npy'):\n",
    "            chunk_embeddings = load_embeddings(os.path.join(folder_path, file_name), ids_order)\n",
    "            if chunk_embeddings:\n",
    "                embeddings_dict.update(chunk_embeddings)\n",
    "                print(f\"Loaded {len(chunk_embeddings)} embeddings from {file_name}\")\n",
    "    \n",
    "    print(f\"Total embeddings loaded: {len(embeddings_dict)}\")\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_save_embeddings(docs, ids, EMBEDDINGS_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec888c",
   "metadata": {},
   "source": [
    "# 4. Bertopic modeling by topic and platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "299a6dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 embeddings from embeddings_1.npy\n",
      "Loaded 10000 embeddings from embeddings_2.npy\n",
      "Loaded 10000 embeddings from embeddings_3.npy\n",
      "Loaded 10000 embeddings from embeddings_4.npy\n",
      "Loaded 10000 embeddings from embeddings_5.npy\n",
      "Loaded 10000 embeddings from embeddings_6.npy\n",
      "Loaded 10000 embeddings from embeddings_7.npy\n",
      "Loaded 10000 embeddings from embeddings_8.npy\n",
      "Loaded 10000 embeddings from embeddings_9.npy\n",
      "Loaded 10000 embeddings from embeddings_10.npy\n",
      "Loaded 10000 embeddings from embeddings_11.npy\n",
      "Loaded 10000 embeddings from embeddings_12.npy\n",
      "Loaded 10000 embeddings from embeddings_13.npy\n",
      "Loaded 10000 embeddings from embeddings_14.npy\n",
      "Loaded 10000 embeddings from embeddings_15.npy\n",
      "Loaded 10000 embeddings from embeddings_16.npy\n",
      "Loaded 10000 embeddings from embeddings_17.npy\n",
      "Loaded 10000 embeddings from embeddings_18.npy\n",
      "Loaded 10000 embeddings from embeddings_19.npy\n",
      "Loaded 10000 embeddings from embeddings_20.npy\n",
      "Loaded 10000 embeddings from embeddings_21.npy\n",
      "Loaded 10000 embeddings from embeddings_22.npy\n",
      "Loaded 10000 embeddings from embeddings_23.npy\n",
      "Loaded 10000 embeddings from embeddings_24.npy\n",
      "Loaded 10000 embeddings from embeddings_25.npy\n",
      "Loaded 10000 embeddings from embeddings_26.npy\n",
      "Loaded 10000 embeddings from embeddings_27.npy\n",
      "Loaded 10000 embeddings from embeddings_28.npy\n",
      "Loaded 10000 embeddings from embeddings_29.npy\n",
      "Loaded 10000 embeddings from embeddings_30.npy\n",
      "Loaded 10000 embeddings from embeddings_31.npy\n",
      "Loaded 10000 embeddings from embeddings_32.npy\n",
      "Loaded 10000 embeddings from embeddings_33.npy\n",
      "Loaded 10000 embeddings from embeddings_34.npy\n",
      "Loaded 10000 embeddings from embeddings_35.npy\n",
      "Loaded 10000 embeddings from embeddings_36.npy\n",
      "Loaded 10000 embeddings from embeddings_37.npy\n",
      "Loaded 10000 embeddings from embeddings_38.npy\n",
      "Loaded 10000 embeddings from embeddings_39.npy\n",
      "Loaded 10000 embeddings from embeddings_40.npy\n",
      "Loaded 10000 embeddings from embeddings_41.npy\n",
      "Loaded 10000 embeddings from embeddings_42.npy\n",
      "Loaded 10000 embeddings from embeddings_43.npy\n",
      "Loaded 10000 embeddings from embeddings_44.npy\n",
      "Loaded 10000 embeddings from embeddings_45.npy\n",
      "Loaded 10000 embeddings from embeddings_46.npy\n",
      "Loaded 10000 embeddings from embeddings_47.npy\n",
      "Loaded 10000 embeddings from embeddings_48.npy\n",
      "Loaded 10000 embeddings from embeddings_49.npy\n",
      "Loaded 10000 embeddings from embeddings_50.npy\n",
      "Loaded 10000 embeddings from embeddings_51.npy\n",
      "Loaded 10000 embeddings from embeddings_52.npy\n",
      "Loaded 10000 embeddings from embeddings_53.npy\n",
      "Loaded 10000 embeddings from embeddings_54.npy\n",
      "Loaded 10000 embeddings from embeddings_55.npy\n",
      "Loaded 4943 embeddings from embeddings_56.npy\n",
      "Total embeddings loaded: 554943\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "embeddings = load_all_embeddings(EMBEDDINGS_OUTPUT_PATH, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from umap import UMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any\n",
    "def analyze_topic_platform(df: pd.DataFrame, \n",
    "                         topic: str, \n",
    "                         platform: str, \n",
    "                         embeddings_dict: Dict[str, np.ndarray],\n",
    "                         min_cluster_size: int = 20) -> tuple:\n",
    "    \"\"\"\n",
    "    Analyze documents for a specific topic and platform combination.\n",
    "    Returns topic model and document embeddings.\n",
    "    \"\"\"\n",
    "    # Filter data\n",
    "    mask = (df['topic'] == topic) & (df['platform'] == platform)\n",
    "    subset_df = df[mask].copy()\n",
    "    \n",
    "    # Print diagnostic information\n",
    "    print(f\"Found {len(subset_df)} documents for {topic} on {platform}\")\n",
    "    \n",
    "    # Filter for documents that have embeddings\n",
    "    subset_df = subset_df[subset_df['embed_id'].astype(str).isin(embeddings_dict.keys())].copy()\n",
    "    \n",
    "    if len(subset_df) < min_cluster_size:\n",
    "        print(f\"Insufficient data for {topic} on {platform}: {len(subset_df)} documents\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Get documents and their embeddings\n",
    "    doc_ids = subset_df['embed_id'].astype(str).tolist()\n",
    "    documents = subset_df['post_clean'].tolist()\n",
    "    \n",
    "    print(f\"Processing {len(documents)} documents after filtering\")\n",
    "    \n",
    "    try:\n",
    "        doc_embeddings = np.array([embeddings_dict[id_] for id_ in doc_ids])\n",
    "        \n",
    "        # Initialize models with more conservative parameters\n",
    "        umap_model = UMAP(\n",
    "            n_neighbors=min(15, len(documents)-1),  # Ensure n_neighbors is less than n_samples\n",
    "            n_components=2,\n",
    "            min_dist=0.1,\n",
    "            metric='cosine',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        vectorizer_model = CountVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=2  # At least 2 documents must contain the term\n",
    "        )\n",
    "        \n",
    "        # Initialize BERTopic with more robust parameters\n",
    "        topic_model = BERTopic(\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            min_topic_size=max(min_cluster_size, 5),  # Ensure minimum size is reasonable\n",
    "            nr_topics=\"auto\",\n",
    "            calculate_probabilities=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        topics, _ = topic_model.fit_transform(documents, doc_embeddings)\n",
    "        \n",
    "        # Check if any topics were found\n",
    "        if len(topic_model.get_topic_info()) <= 1:  # Only -1 topic means no clusters found\n",
    "            print(f\"No meaningful topics found for {topic} on {platform}\")\n",
    "            return None, None, None\n",
    "            \n",
    "        # Reduce embeddings for visualization\n",
    "        reduced_embeddings = umap_model.fit_transform(doc_embeddings)\n",
    "        \n",
    "        return topic_model, documents, reduced_embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {topic} on {platform}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "def visualize_results(topic_model: BERTopic, \n",
    "                     documents: List[str], \n",
    "                     reduced_embeddings: np.ndarray,\n",
    "                     topic: str,\n",
    "                     platform: str) -> None:\n",
    "    \"\"\"\n",
    "    Create and save visualizations for topic modeling results.\n",
    "    \"\"\"\n",
    "    # Get topic info\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    top_topics = topic_info[topic_info['Topic'] != -1].head(10)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot each topic\n",
    "    unique_topics = top_topics['Topic'].tolist()\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_topics)))\n",
    "    \n",
    "    # Plot points\n",
    "    for idx, t in enumerate(unique_topics):\n",
    "        mask = np.array(topic_model.topics_) == t\n",
    "        if np.any(mask):\n",
    "            points = reduced_embeddings[mask]\n",
    "            # Get top 10 words for this topic\n",
    "            words = topic_model.get_topic(t)[:10]\n",
    "            label = '_'.join([word[0] for word in words])\n",
    "            plt.scatter(points[:, 0], points[:, 1], \n",
    "                       c=[colors[idx]], \n",
    "                       label=f\"Topic {t}: {label}\",\n",
    "                       alpha=0.6, s=20)\n",
    "    \n",
    "    plt.title(f'Topic Distribution: {topic} on {platform}')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f'/VData/scro4316/ct_prevalence/results/topic_viz_{topic}_{platform}.png', \n",
    "                bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33d7cdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for each topic-platform combination...\n",
      "\n",
      "Analyzing KEYWORDS_COVID19 on X\n",
      "==================================================\n",
      "Found 34998 documents for KEYWORDS_COVID19 on X\n",
      "Processing 34900 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:14:22,916 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-03 12:14:50,162 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 12:14:50,165 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 12:15:59,790 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 12:15:59,793 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 12:16:02,326 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 12:16:02,329 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 12:16:02,408 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 12:16:04,675 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 12:16:04,678 - BERTopic - Topic reduction - Reduced number of topics from 220 to 4\n",
      "2025-11-03 12:16:34,927 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_COVID19 on X:\n",
      "Total documents: 34900\n",
      "\n",
      "Top 10 topics:\n",
      "   Topic  Count                                           Name\n",
      "1      0  18817               0_covid_vaccine_vaccines_covid19\n",
      "2      1     96                                 1_la_il_di_che\n",
      "3      2     25  2_english_golden_vaccine created_like covid19\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_COVID19 on fediverse\n",
      "==================================================\n",
      "Found 6096 documents for KEYWORDS_COVID19 on fediverse\n",
      "Processing 6082 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:16:43,169 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 12:16:43,170 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 12:16:43,873 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 12:16:43,874 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 12:16:45,250 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 12:16:45,252 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 12:16:45,266 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 12:16:46,491 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 12:16:46,494 - BERTopic - Topic reduction - Reduced number of topics from 64 to 28\n",
      "2025-11-03 12:16:55,017 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_COVID19 on fediverse:\n",
      "Total documents: 6082\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                         Name\n",
      "1       0   1949             0_covid_vaccine_vaccines_covid19\n",
      "2       1    125                    1_woke_patriot_says_trump\n",
      "3       2    124                      2_jews_lie_russians_jew\n",
      "4       3    115                  3_usaid_funded_musk_funding\n",
      "5       4     66  4_scamdemic_cdc grooming_grooming_new world\n",
      "6       5     65               5_economy_assets_wealth_market\n",
      "7       6     56        6_canada_canadian_minister_government\n",
      "8       7     52          7_ivermectin_cancer_drug_remdesivir\n",
      "9       8     49        8_covid_memes_covid1984_covid covid19\n",
      "10      9     48       9_cancer_cancers_aggressive_soonshiong\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_COVID19 on bluesky\n",
      "==================================================\n",
      "Found 27487 documents for KEYWORDS_COVID19 on bluesky\n",
      "Processing 27467 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:17:14,835 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 12:17:14,836 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 12:17:27,545 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 12:17:27,546 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 12:17:29,205 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 12:17:29,206 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 12:17:29,236 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 12:17:30,849 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 12:17:30,852 - BERTopic - Topic reduction - Reduced number of topics from 122 to 55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_COVID19 on bluesky:\n",
      "Total documents: 27467\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                          Name\n",
      "1       0  11054               0_covid_people_vaccines_vaccine\n",
      "2       1    384                     1_masks_mask_wear_wearing\n",
      "3       2    276                     2_uk_tories_labour_brexit\n",
      "4       3    176                         3_shes_covid_lady_did\n",
      "5       4    172        4_fauci_anthony_anthony fauci_function\n",
      "6       5    163                  5_musk_elon_trump_trump musk\n",
      "7       6    116               6_canada_trudeau_ford_canadians\n",
      "8       7    104  7_facebook_zuckerberg_censor_mark zuckerberg\n",
      "9       8    103  8_plandemic_real plandemic_real_plandemic 20\n",
      "10      9     98                  9_ballots_2020_election_mail\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_COVID19 on truthsocial\n",
      "==================================================\n",
      "Found 98878 documents for KEYWORDS_COVID19 on truthsocial\n",
      "Processing 98799 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:17:53,926 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-03 12:21:28,204 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 12:21:28,208 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 16:53:03,482 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 16:53:03,496 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 16:53:16,055 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 16:53:16,060 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 16:53:17,338 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 16:53:28,543 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 16:53:28,556 - BERTopic - Topic reduction - Reduced number of topics from 1017 to 811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_COVID19 on truthsocial:\n",
      "Total documents: 98799\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                                                 Name\n",
      "1       0   1583                                             0_china_chinese_ccp_land\n",
      "2       1   1414  1_involved_collusion hoax_involved russia_involved russia collusion\n",
      "3       2   1243                                          2_hes_heaposs_needs_nursing\n",
      "4       3   1114                                      3_mrna_mortality_studies_excess\n",
      "5       4   1024                                                 4_gop_zero_math_dems\n",
      "6       5    965                                    5_fauci_dr fauci_aids_fauci needs\n",
      "7       6    952                                                  6_flu_came_lab_died\n",
      "8       7    935                                         7_bird_bird flu_chickens_flu\n",
      "9       8    822               8_zuckerberg_ad_rockefeller foundation_mark zuckerberg\n",
      "10      9    747                                              9_shot_got_took_vaccine\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_COVID19 on 4chan\n",
      "==================================================\n",
      "Found 16369 documents for KEYWORDS_COVID19 on 4chan\n",
      "Processing 16358 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 16:55:40,476 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-03 16:55:51,408 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 16:55:51,411 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 16:55:53,495 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 16:55:53,496 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 16:55:55,733 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 16:55:55,735 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 16:55:55,747 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 16:55:57,854 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 16:55:57,858 - BERTopic - Topic reduction - Reduced number of topics from 61 to 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_COVID19 on 4chan:\n",
      "Total documents: 16358\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                 Name\n",
      "1       0   5273          0_covid_vaccine_people_just\n",
      "2       1    627              1_jews_jewish_jew_covid\n",
      "3       2    523       2_clot_clot shot_shot_clotshot\n",
      "4       3    232           3_ukraine_russia_war_putin\n",
      "5       4    223              4_people_shit_like_just\n",
      "6       5    201             5_eggs_chickens_egg_bird\n",
      "7       6    175  6_canada_canadians_trudeau_canadian\n",
      "8       7    154             7_women_white_men_people\n",
      "9       8    128              8_god_beast_mark_christ\n",
      "10      9    113                  9_5g_cloud_ngo_able\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_COVID19 on gettr\n",
      "==================================================\n",
      "Found 28834 documents for KEYWORDS_COVID19 on gettr\n",
      "Processing 28762 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 16:56:09,054 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-03 16:56:31,625 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 16:56:31,627 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 17:00:24,049 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 17:00:24,052 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 17:00:28,070 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 17:00:28,075 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 17:00:28,283 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 17:00:32,258 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 17:00:32,262 - BERTopic - Topic reduction - Reduced number of topics from 377 to 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_COVID19 on gettr:\n",
      "Total documents: 28762\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                           Name\n",
      "1       0  17830               0_covid_vaccine_covid19_vaccines\n",
      "2       1    105                       1_jews_israel_jewish_jew\n",
      "3       2     95    2_plandemic_plan_plandemic 20_new plandemic\n",
      "4       3     74                   3_newsom_california_la_gavin\n",
      "5       4     74                 4_dangerous_uk_government_just\n",
      "6       5     65              5_birx_covid_deborah birx_deborah\n",
      "7       6     60              6_carney_canada_trudeau_canadians\n",
      "8       7     57       7_prosecuted_fully_willful_fully exposed\n",
      "9       8     55  8_davos_party_davos party_virus covid vaccine\n",
      "10      9     50                 9_pope_church_vatican_catholic\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_COVID19 on gab\n",
      "==================================================\n",
      "Found 62795 documents for KEYWORDS_COVID19 on gab\n",
      "Processing 62634 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 17:00:53,074 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-03 17:01:42,579 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 17:01:42,582 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 17:44:13,829 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 17:44:13,831 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 17:44:30,052 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 17:44:30,060 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 17:44:30,731 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 17:44:46,558 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 17:44:46,568 - BERTopic - Topic reduction - Reduced number of topics from 693 to 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_COVID19 on gab:\n",
      "Total documents: 62634\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                                     Name\n",
      "1       0  39514                             0_covid_people_vaccine_trump\n",
      "2       1    779                             1_born_prime_nonprime_verses\n",
      "3       2    134                        2_clotshot_dies_age_notabledeaths\n",
      "4       3    124                       3_cabal_obsidian_digital euro_usdr\n",
      "5       4    111  4_whats_trump2020_socialist_national security apparatus\n",
      "6       5     62                                5_dea_fbi_news_precursors\n",
      "7       6     53                           6_lesson_ninja_ninja kids_rick\n",
      "8       7     51                     7_aluminum_rifles_243_243 winchester\n",
      "9       8     50                         8_uk_globalist_pakistani_muslims\n",
      "10      9     46             9_said_nothingwhen_said nothingwhen_illegals\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_NWO on X\n",
      "==================================================\n",
      "Found 32517 documents for KEYWORDS_NWO on X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 17:45:32,403 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 32406 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 17:45:56,145 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 17:45:56,147 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 17:46:27,843 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 17:46:27,846 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 17:46:30,241 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 17:46:30,244 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 17:46:30,307 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 17:46:32,503 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 17:46:32,507 - BERTopic - Topic reduction - Reduced number of topics from 191 to 133\n",
      "2025-11-03 17:46:51,911 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_NWO on X:\n",
      "Total documents: 32406\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                                              Name\n",
      "1       0   1477                          0_illuminati_globalist_illuminatiam_join\n",
      "2       1   1123                         1_freemasonry_freemasons_freemason_israel\n",
      "3       2   1060  2_depopulation_depopulation agenda_population_population control\n",
      "4       3    721                                     3_obama_deep state_deep_biden\n",
      "5       4    695                                4_canada_carney_canadians_canadian\n",
      "6       5    544                   5_wwg1wga_ncswic_wwg1wga wwg1wga_ncswic wwg1wga\n",
      "7       6    540                              6_nwo_antichrist_nwo nwo_welcome nwo\n",
      "8       7    485                             7_psyop_psyop psyop_psyop just_psyops\n",
      "9       8    427                             8_reset_great reset_great_reset great\n",
      "10      9    398                                       9_elon_musk_trump_elon musk\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_NWO on fediverse\n",
      "==================================================\n",
      "Found 2849 documents for KEYWORDS_NWO on fediverse\n",
      "Processing 2845 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 17:47:02,512 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 17:47:02,514 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 17:47:02,670 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 17:47:02,671 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 17:47:03,201 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 17:47:03,202 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 17:47:03,209 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 17:47:03,674 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 17:47:03,676 - BERTopic - Topic reduction - Reduced number of topics from 31 to 17\n",
      "2025-11-03 17:47:14,659 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_NWO on fediverse:\n",
      "Total documents: 2845\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                                  Name\n",
      "1       0   1071                         0_world_new_order_world order\n",
      "2       1    171                         1_vaccines_mrna_covid_covid19\n",
      "3       2    137  2_human blood_blood sacrifice_real illuminati_helped\n",
      "4       3    130      3_illuminati_freemasonry_freemasons_organization\n",
      "5       4    130  4_population_depopulation_control_population control\n",
      "6       5     74                         5_psyop_jfk_cia_assassination\n",
      "7       6     56                      6_musk_elon_elon musk_technology\n",
      "8       7     46                       7_reset_great reset_great_going\n",
      "9       8     43                        8_canada_carney_mark carney_eu\n",
      "10      9     35                  9_globohomo_science_truth_reptilians\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_NWO on bluesky\n",
      "==================================================\n",
      "Found 13094 documents for KEYWORDS_NWO on bluesky\n",
      "Processing 13082 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 17:47:24,234 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 17:47:24,236 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 17:47:27,573 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 17:47:27,574 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 17:47:28,277 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 17:47:28,278 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 17:47:28,297 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 17:47:28,937 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 17:47:28,940 - BERTopic - Topic reduction - Reduced number of topics from 92 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_NWO on bluesky:\n",
      "Total documents: 13082\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                                                      Name\n",
      "1       0   2535                             0_world order_order_new world_new world order\n",
      "2       1   1095                             1_illuminati_whatsapp_great illuminati_member\n",
      "3       2    897                            2_population_population control_control_people\n",
      "4       3    777                   3_controlled opposition_controlled_opposition_democrats\n",
      "5       4    602  4_freemason scheme_democracy freemason_democracy freemason scheme_scheme\n",
      "6       5    258                                   5_nwo_fuse_long_secret world government\n",
      "7       6    240                              6_freemasons_freemasonry_demon_strange oaths\n",
      "8       7    192                                    7_psyop_government psyop_cia_cia psyop\n",
      "9       8    188                                            8_israel_gaza_genocide_zionist\n",
      "10      9    176                                         9_hes_world_world order_new world\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_NWO on truthsocial\n",
      "==================================================\n",
      "Found 76189 documents for KEYWORDS_NWO on truthsocial\n",
      "Processing 76107 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 17:47:38,148 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-03 17:48:49,984 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 17:48:49,989 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 18:51:27,792 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 18:51:27,796 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 18:51:37,898 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 18:51:37,905 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 18:51:38,622 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 18:51:48,539 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 18:51:48,549 - BERTopic - Topic reduction - Reduced number of topics from 733 to 538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_NWO on truthsocial:\n",
      "Total documents: 76107\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                                             Name\n",
      "1       0   5014                                 0_paper_ballots_people need_hand\n",
      "2       1   2438                       1_world order_world_world government_order\n",
      "3       2   2124               2_population_depopulation_population control_gates\n",
      "4       3   1601                                  3_putin_ukraine_russia_zelensky\n",
      "5       4   1346  4_amen thank wwg1wga_amen thank_wwg1wga amen thank_wwg1wga amen\n",
      "6       5   1340                                         5_jews_jewish_israel_jew\n",
      "7       6   1222                                          6_nwo_nwo nwo_rt_nwo rt\n",
      "8       7    970                    7_freemasons_freemason_freemasonry_illuminati\n",
      "9       8    903                                          8_smart_fires_la_cities\n",
      "10      9    687                             9_canada_carney_north_north american\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_NWO on 4chan\n",
      "==================================================\n",
      "Found 33980 documents for KEYWORDS_NWO on 4chan\n",
      "Processing 33948 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 18:52:58,351 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-03 18:53:21,704 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 18:53:21,708 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 18:54:05,211 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 18:54:05,215 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 18:54:09,401 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 18:54:09,404 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 18:54:09,472 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 18:54:13,484 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 18:54:13,489 - BERTopic - Topic reduction - Reduced number of topics from 196 to 118\n",
      "2025-11-03 18:54:34,969 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_NWO on 4chan:\n",
      "Total documents: 33948\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                                               Name\n",
      "1       0   4973                                     0_jews_jewish_freemasons_psyop\n",
      "2       1   2911                                      1_ukraine_russia_nato_russian\n",
      "3       2    574                                          2_china_japan_chinese_ccp\n",
      "4       3    535                                     3_covid_vaccine_vaccines_virus\n",
      "5       4    514  4_ai_total surveillance_killing jews politicians_jews politicians\n",
      "6       5    380                                          5_illuminati_im_know_like\n",
      "7       6    307                                 6_canada_carney_canadian_canadians\n",
      "8       7    304                                        7_elon_musk_trump_elon musk\n",
      "9       8    289          8_white men_public enemy number_public enemy_enemy number\n",
      "10      9    288   9_population_population control_depopulation_depopulation agenda\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_NWO on gettr\n",
      "==================================================\n",
      "Found 11556 documents for KEYWORDS_NWO on gettr\n",
      "Processing 11536 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 18:54:42,849 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 18:54:42,852 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 18:54:46,181 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 18:54:46,182 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 18:54:48,623 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 18:54:48,626 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 18:54:48,647 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 18:54:50,961 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 18:54:50,965 - BERTopic - Topic reduction - Reduced number of topics from 98 to 33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_NWO on gettr:\n",
      "Total documents: 11536\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                               Name\n",
      "1       0   5737          0_world_trump_people_just\n",
      "2       1    338             1_nwo_sb_divided_trump\n",
      "3       2    313     2_communist_fires_smart_cities\n",
      "4       3    311  3_yhwh_swamp_america_swamp combat\n",
      "5       4    240  4_canada_carney_trudeau_greenland\n",
      "6       5    192      5_ukraine_zelensky_war_russia\n",
      "7       6    174        6_elon_musk_elon musk_musks\n",
      "8       7     92        7_shes_world_eu_world order\n",
      "9       8     76         8_women_men_trans_children\n",
      "10      9     71            9_ccp_china_chinese_guo\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_NWO on gab\n",
      "==================================================\n",
      "Found 48373 documents for KEYWORDS_NWO on gab\n",
      "Processing 48303 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 18:54:58,885 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-03 18:55:35,746 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 18:55:35,748 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 19:05:42,743 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 19:05:42,746 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 19:05:55,713 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 19:05:55,719 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 19:05:56,041 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 19:06:09,141 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 19:06:09,151 - BERTopic - Topic reduction - Reduced number of topics from 491 to 329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_NWO on gab:\n",
      "Total documents: 48303\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                                     Name\n",
      "1       0   2626                          0_zionist_zionist jews_jews_usa\n",
      "2       1   1764                       1_honoring_noahide_kushner_deceive\n",
      "3       2   1107                            2_nwo_nwo wants_trump nwo_hes\n",
      "4       3   1064                  3_climate_climate change_science_change\n",
      "5       4   1010                           4_vaccines_gates_vaccine_fauci\n",
      "6       5    784                  5_pope_catholic_vatican_catholic church\n",
      "7       6    724  6_hollywood illuminati_hollywood_illuminati_deep church\n",
      "8       7    706       7_trump2020_socialist_stopthesteal_trump trump2020\n",
      "9       8    543                            8_north_screen_china_japanese\n",
      "10      9    477                            9_white_whites_racism_kalergi\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each topic-platform combination\n",
    "# topics = df['topic'].unique().tolist()\n",
    "# topics = ['KEYWORDS_9_11','KEYWORDS_ALIEN', 'KEYWORDS_MOON']\n",
    "topics = ['KEYWORDS_COVID19','KEYWORDS_NWO']\n",
    "\n",
    "platforms = df['platform'].unique().tolist()\n",
    "print(\"Starting analysis for each topic-platform combination...\")\n",
    "results_summary = []\n",
    "\n",
    "for topic in topics:\n",
    "    for platform in platforms:\n",
    "        print(f\"\\nAnalyzing {topic} on {platform}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            topic_model, documents, reduced_embeddings = analyze_topic_platform(\n",
    "                df, topic, platform, embeddings\n",
    "            )\n",
    "            \n",
    "            if topic_model is not None and documents is not None:\n",
    "                # Get and display topic information\n",
    "                topic_info = topic_model.get_topic_info()\n",
    "                if len(topic_info) > 1:  # More than just the -1 topic\n",
    "                    top_topics = topic_info[topic_info['Topic'] != -1].head(10)\n",
    "                    \n",
    "                    # Store results\n",
    "                    results_summary.append({\n",
    "                        'topic': topic,\n",
    "                        'platform': platform,\n",
    "                        'n_documents': len(documents),\n",
    "                        'top_topics': top_topics\n",
    "                    })\n",
    "                    \n",
    "                    # Create visualization\n",
    "                    visualize_results(topic_model, documents, reduced_embeddings, topic, platform)\n",
    "                    \n",
    "                    # Print summary\n",
    "                    print(f\"\\nResults for {topic} on {platform}:\")\n",
    "                    print(f\"Total documents: {len(documents)}\")\n",
    "                    print(\"\\nTop 10 topics:\")\n",
    "                    print(top_topics[['Topic', 'Count', 'Name']].to_string())\n",
    "                else:\n",
    "                    print(f\"No meaningful topics found for {topic} on {platform}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {topic} on {platform}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b55214fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "Topic: KEYWORDS_COVID19\n",
      "----------------------------------------\n",
      "                                                                   X  \\\n",
      "Topic_Num                                                              \n",
      "Topic 0              [0, covid, vaccine, vaccines, covid19], n=18817   \n",
      "Topic 1                                   [1, la, il, di, che], n=96   \n",
      "Topic 2    [2, english, golden, vaccine created, like covid19], n=25   \n",
      "Topic 3                                                         null   \n",
      "Topic 4                                                         null   \n",
      "Topic 5                                                         null   \n",
      "Topic 6                                                         null   \n",
      "Topic 7                                                         null   \n",
      "Topic 8                                                         null   \n",
      "Topic 9                                                         null   \n",
      "\n",
      "                                                         fediverse  \\\n",
      "Topic_Num                                                            \n",
      "Topic 0             [0, covid, vaccine, vaccines, covid19], n=1949   \n",
      "Topic 1                     [1, woke, patriot, says, trump], n=125   \n",
      "Topic 2                       [2, jews, lie, russians, jew], n=124   \n",
      "Topic 3                   [3, usaid, funded, musk, funding], n=115   \n",
      "Topic 4    [4, scamdemic, cdc grooming, grooming, new world], n=66   \n",
      "Topic 5                 [5, economy, assets, wealth, market], n=65   \n",
      "Topic 6          [6, canada, canadian, minister, government], n=56   \n",
      "Topic 7            [7, ivermectin, cancer, drug, remdesivir], n=52   \n",
      "Topic 8          [8, covid, memes, covid1984, covid covid19], n=49   \n",
      "Topic 9         [9, cancer, cancers, aggressive, soonshiong], n=48   \n",
      "\n",
      "                                                             bluesky  \\\n",
      "Topic_Num                                                              \n",
      "Topic 0               [0, covid, people, vaccines, vaccine], n=11054   \n",
      "Topic 1                       [1, masks, mask, wear, wearing], n=384   \n",
      "Topic 2                       [2, uk, tories, labour, brexit], n=276   \n",
      "Topic 3                           [3, shes, covid, lady, did], n=176   \n",
      "Topic 4          [4, fauci, anthony, anthony fauci, function], n=172   \n",
      "Topic 5                    [5, musk, elon, trump, trump musk], n=163   \n",
      "Topic 6                 [6, canada, trudeau, ford, canadians], n=116   \n",
      "Topic 7    [7, facebook, zuckerberg, censor, mark zuckerberg], n=104   \n",
      "Topic 8    [8, plandemic, real plandemic, real, plandemic 20], n=103   \n",
      "Topic 9                     [9, ballots, 2020, election, mail], n=98   \n",
      "\n",
      "                                                                                 truthsocial  \\\n",
      "Topic_Num                                                                                      \n",
      "Topic 0                                               [0, china, chinese, ccp, land], n=1583   \n",
      "Topic 1    [1, involved, collusion hoax, involved russia, involved russia collusion], n=1414   \n",
      "Topic 2                                            [2, hes, heaposs, needs, nursing], n=1243   \n",
      "Topic 3                                        [3, mrna, mortality, studies, excess], n=1114   \n",
      "Topic 4                                                   [4, gop, zero, math, dems], n=1024   \n",
      "Topic 5                                       [5, fauci, dr fauci, aids, fauci needs], n=965   \n",
      "Topic 6                                                     [6, flu, came, lab, died], n=952   \n",
      "Topic 7                                            [7, bird, bird flu, chickens, flu], n=935   \n",
      "Topic 8                  [8, zuckerberg, ad, rockefeller foundation, mark zuckerberg], n=822   \n",
      "Topic 9                                                 [9, shot, got, took, vaccine], n=747   \n",
      "\n",
      "                                                      4chan  \\\n",
      "Topic_Num                                                     \n",
      "Topic 0           [0, covid, vaccine, people, just], n=5273   \n",
      "Topic 1                [1, jews, jewish, jew, covid], n=627   \n",
      "Topic 2         [2, clot, clot shot, shot, clotshot], n=523   \n",
      "Topic 3             [3, ukraine, russia, war, putin], n=232   \n",
      "Topic 4                [4, people, shit, like, just], n=223   \n",
      "Topic 5               [5, eggs, chickens, egg, bird], n=201   \n",
      "Topic 6    [6, canada, canadians, trudeau, canadian], n=175   \n",
      "Topic 7               [7, women, white, men, people], n=154   \n",
      "Topic 8                [8, god, beast, mark, christ], n=128   \n",
      "Topic 9                    [9, 5g, cloud, ngo, able], n=113   \n",
      "\n",
      "                                                               gettr  \\\n",
      "Topic_Num                                                              \n",
      "Topic 0              [0, covid, vaccine, covid19, vaccines], n=17830   \n",
      "Topic 1                        [1, jews, israel, jewish, jew], n=105   \n",
      "Topic 2      [2, plandemic, plan, plandemic 20, new plandemic], n=95   \n",
      "Topic 3                     [3, newsom, california, la, gavin], n=74   \n",
      "Topic 4                   [4, dangerous, uk, government, just], n=74   \n",
      "Topic 5                [5, birx, covid, deborah birx, deborah], n=65   \n",
      "Topic 6                [6, carney, canada, trudeau, canadians], n=60   \n",
      "Topic 7         [7, prosecuted, fully, willful, fully exposed], n=57   \n",
      "Topic 8    [8, davos, party, davos party, virus covid vaccine], n=55   \n",
      "Topic 9                   [9, pope, church, vatican, catholic], n=50   \n",
      "\n",
      "                                                                            gab  \n",
      "Topic_Num                                                                        \n",
      "Topic 0                             [0, covid, people, vaccine, trump], n=39514  \n",
      "Topic 1                               [1, born, prime, nonprime, verses], n=779  \n",
      "Topic 2                          [2, clotshot, dies, age, notabledeaths], n=134  \n",
      "Topic 3                         [3, cabal, obsidian, digital euro, usdr], n=124  \n",
      "Topic 4    [4, whats, trump2020, socialist, national security apparatus], n=111  \n",
      "Topic 5                                   [5, dea, fbi, news, precursors], n=62  \n",
      "Topic 6                              [6, lesson, ninja, ninja kids, rick], n=53  \n",
      "Topic 7                        [7, aluminum, rifles, 243, 243 winchester], n=51  \n",
      "Topic 8                            [8, uk, globalist, pakistani, muslims], n=50  \n",
      "Topic 9                [9, said, nothingwhen, said nothingwhen, illegals], n=46  \n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Topic: KEYWORDS_NWO\n",
      "----------------------------------------\n",
      "                                                                                        X  \\\n",
      "Topic_Num                                                                                   \n",
      "Topic 0                            [0, illuminati, globalist, illuminatiam, join], n=1477   \n",
      "Topic 1                           [1, freemasonry, freemasons, freemason, israel], n=1123   \n",
      "Topic 2    [2, depopulation, depopulation agenda, population, population control], n=1060   \n",
      "Topic 3                                        [3, obama, deep state, deep, biden], n=721   \n",
      "Topic 4                                   [4, canada, carney, canadians, canadian], n=695   \n",
      "Topic 5                      [5, wwg1wga, ncswic, wwg1wga wwg1wga, ncswic wwg1wga], n=544   \n",
      "Topic 6                                 [6, nwo, antichrist, nwo nwo, welcome nwo], n=540   \n",
      "Topic 7                                [7, psyop, psyop psyop, psyop just, psyops], n=485   \n",
      "Topic 8                                [8, reset, great reset, great, reset great], n=427   \n",
      "Topic 9                                          [9, elon, musk, trump, elon musk], n=398   \n",
      "\n",
      "                                                                   fediverse  \\\n",
      "Topic_Num                                                                      \n",
      "Topic 0                          [0, world, new, order, world order], n=1071   \n",
      "Topic 1                           [1, vaccines, mrna, covid, covid19], n=171   \n",
      "Topic 2    [2, human blood, blood sacrifice, real illuminati, helped], n=137   \n",
      "Topic 3        [3, illuminati, freemasonry, freemasons, organization], n=130   \n",
      "Topic 4    [4, population, depopulation, control, population control], n=130   \n",
      "Topic 5                            [5, psyop, jfk, cia, assassination], n=74   \n",
      "Topic 6                         [6, musk, elon, elon musk, technology], n=56   \n",
      "Topic 7                          [7, reset, great reset, great, going], n=46   \n",
      "Topic 8                           [8, canada, carney, mark carney, eu], n=43   \n",
      "Topic 9                     [9, globohomo, science, truth, reptilians], n=35   \n",
      "\n",
      "                                                                                         bluesky  \\\n",
      "Topic_Num                                                                                          \n",
      "Topic 0                              [0, world order, order, new world, new world order], n=2535   \n",
      "Topic 1                              [1, illuminati, whatsapp, great illuminati, member], n=1095   \n",
      "Topic 2                              [2, population, population control, control, people], n=897   \n",
      "Topic 3                     [3, controlled opposition, controlled, opposition, democrats], n=777   \n",
      "Topic 4    [4, freemason scheme, democracy freemason, democracy freemason scheme, scheme], n=602   \n",
      "Topic 5                                     [5, nwo, fuse, long, secret world government], n=258   \n",
      "Topic 6                                [6, freemasons, freemasonry, demon, strange oaths], n=240   \n",
      "Topic 7                                      [7, psyop, government psyop, cia, cia psyop], n=192   \n",
      "Topic 8                                              [8, israel, gaza, genocide, zionist], n=188   \n",
      "Topic 9                                           [9, hes, world, world order, new world], n=176   \n",
      "\n",
      "                                                                             truthsocial  \\\n",
      "Topic_Num                                                                                  \n",
      "Topic 0                                   [0, paper, ballots, people need, hand], n=5014   \n",
      "Topic 1                         [1, world order, world, world government, order], n=2438   \n",
      "Topic 2                 [2, population, depopulation, population control, gates], n=2124   \n",
      "Topic 3                                    [3, putin, ukraine, russia, zelensky], n=1601   \n",
      "Topic 4    [4, amen thank wwg1wga, amen thank, wwg1wga amen thank, wwg1wga amen], n=1346   \n",
      "Topic 5                                           [5, jews, jewish, israel, jew], n=1340   \n",
      "Topic 6                                            [6, nwo, nwo nwo, rt, nwo rt], n=1222   \n",
      "Topic 7                       [7, freemasons, freemason, freemasonry, illuminati], n=970   \n",
      "Topic 8                                             [8, smart, fires, la, cities], n=903   \n",
      "Topic 9                                [9, canada, carney, north, north american], n=687   \n",
      "\n",
      "                                                                                    4chan  \\\n",
      "Topic_Num                                                                                   \n",
      "Topic 0                                      [0, jews, jewish, freemasons, psyop], n=4973   \n",
      "Topic 1                                       [1, ukraine, russia, nato, russian], n=2911   \n",
      "Topic 2                                            [2, china, japan, chinese, ccp], n=574   \n",
      "Topic 3                                       [3, covid, vaccine, vaccines, virus], n=535   \n",
      "Topic 4    [4, ai, total surveillance, killing jews politicians, jews politicians], n=514   \n",
      "Topic 5                                            [5, illuminati, im, know, like], n=380   \n",
      "Topic 6                                   [6, canada, carney, canadian, canadians], n=307   \n",
      "Topic 7                                          [7, elon, musk, trump, elon musk], n=304   \n",
      "Topic 8            [8, white men, public enemy number, public enemy, enemy number], n=289   \n",
      "Topic 9     [9, population, population control, depopulation, depopulation agenda], n=288   \n",
      "\n",
      "                                                    gettr  \\\n",
      "Topic_Num                                                   \n",
      "Topic 0           [0, world, trump, people, just], n=5737   \n",
      "Topic 1               [1, nwo, sb, divided, trump], n=338   \n",
      "Topic 2       [2, communist, fires, smart, cities], n=313   \n",
      "Topic 3    [3, yhwh, swamp, america, swamp combat], n=311   \n",
      "Topic 4    [4, canada, carney, trudeau, greenland], n=240   \n",
      "Topic 5        [5, ukraine, zelensky, war, russia], n=192   \n",
      "Topic 6          [6, elon, musk, elon musk, musks], n=174   \n",
      "Topic 7           [7, shes, world, eu, world order], n=92   \n",
      "Topic 8            [8, women, men, trans, children], n=76   \n",
      "Topic 9               [9, ccp, china, chinese, guo], n=71   \n",
      "\n",
      "                                                                            gab  \n",
      "Topic_Num                                                                        \n",
      "Topic 0                           [0, zionist, zionist jews, jews, usa], n=2626  \n",
      "Topic 1                        [1, honoring, noahide, kushner, deceive], n=1764  \n",
      "Topic 2                             [2, nwo, nwo wants, trump nwo, hes], n=1107  \n",
      "Topic 3                   [3, climate, climate change, science, change], n=1064  \n",
      "Topic 4                            [4, vaccines, gates, vaccine, fauci], n=1010  \n",
      "Topic 5                    [5, pope, catholic, vatican, catholic church], n=784  \n",
      "Topic 6    [6, hollywood illuminati, hollywood, illuminati, deep church], n=724  \n",
      "Topic 7         [7, trump2020, socialist, stopthesteal, trump trump2020], n=706  \n",
      "Topic 8                              [8, north, screen, china, japanese], n=543  \n",
      "Topic 9                              [9, white, whites, racism, kalergi], n=477  \n",
      "\n",
      "\n",
      "Results have been saved to: /VData/scro4316/ct_prevalence/results\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "results_dir = Path(\"/VData/scro4316/ct_prevalence/results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Store results for each topic-platform combination\n",
    "topic_platform_results = {}\n",
    "\n",
    "# First, save individual results as CSVs\n",
    "for result in results_summary:\n",
    "    topic = result['topic']\n",
    "    platform = result['platform']\n",
    "    top_topics_df = result['top_topics']\n",
    "    \n",
    "    # Add representative words column\n",
    "    top_topics_df['Rep_Words'] = top_topics_df['Name'].apply(lambda x: ', '.join(x.split('_')[:10]))\n",
    "    top_topics_df['Doc_Count'] = top_topics_df['Count']\n",
    "    \n",
    "    # Save to CSV\n",
    "    filename = f\"{topic}_{platform}_topics.csv\"\n",
    "    top_topics_df.to_csv(results_dir / filename)\n",
    "    \n",
    "    # Store in dictionary for table creation\n",
    "    if topic not in topic_platform_results:\n",
    "        topic_platform_results[topic] = {}\n",
    "    topic_platform_results[topic][platform] = {\n",
    "        'topics': top_topics_df['Rep_Words'].tolist(),\n",
    "        'counts': top_topics_df['Count'].tolist()\n",
    "    }\n",
    "\n",
    "topics = ['KEYWORDS_COVID19','KEYWORDS_NWO']\n",
    "# Create comparison tables for each topic\n",
    "for topic in topics:\n",
    "    print(f\"\\n{'-'*40}\")\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(f\"{'-'*40}\")\n",
    "    \n",
    "    # Create DataFrame with platforms as columns\n",
    "    table_data = []\n",
    "    for i in range(10):  # For each of the top 10 topics\n",
    "        row = {'Topic_Num': f'Topic {i}'}\n",
    "        for platform in platforms:\n",
    "            if platform in topic_platform_results.get(topic, {}):\n",
    "                platform_data = topic_platform_results[topic][platform]\n",
    "                if i < len(platform_data['topics']):\n",
    "                    words = platform_data['topics'][i]\n",
    "                    count = platform_data['counts'][i]\n",
    "                    row[platform] = f\"[{words}], n={count}\"\n",
    "                else:\n",
    "                    row[platform] = \"null\"\n",
    "            else:\n",
    "                row[platform] = \"null\"\n",
    "        table_data.append(row)\n",
    "    \n",
    "    # Create and display the table\n",
    "    comparison_df = pd.DataFrame(table_data)\n",
    "    comparison_df.set_index('Topic_Num', inplace=True)\n",
    "    \n",
    "    # Save comparison table\n",
    "    filename = f\"{topic}_platform_comparison.csv\"\n",
    "    comparison_df.to_csv(results_dir / filename)\n",
    "    \n",
    "    # Display the table\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(comparison_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"Results have been saved to:\", results_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ctplatform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
