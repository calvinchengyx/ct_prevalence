{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb0ca6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bertopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61037da5",
   "metadata": {},
   "source": [
    "# 1. Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c007986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset calvin_posts contains: 5490661 records\n",
      "However, 4741220 records are missing values (worth checking) - 86.35% missing rate\n",
      "Platform distribution for bertopic is executed for following data:\n",
      "platform\n",
      "X              279630\n",
      "truthsocial    186506\n",
      "gab            112722\n",
      "4chan           62638\n",
      "bluesky         53242\n",
      "gettr           42091\n",
      "fediverse       12612\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "INPUT = \"/VData/scro4316/ct_prevalence/calvin_posts.parquet\"\n",
    "df = pd.read_parquet(INPUT)\n",
    "total = df['post_text'].shape[0]\n",
    "na_rows = df['post_text'].isna().sum()\n",
    "missing_rate = na_rows / total * 100\n",
    "platform_counts = df['platform'].value_counts()\n",
    "\n",
    "print(f\"The dataset calvin_posts contains: {total} records\")\n",
    "print(f\"However, {na_rows} records are missing values (worth checking) - {missing_rate:.2f}% missing rate\")\n",
    "print(\"Platform distribution for bertopic is executed for following data:\")\n",
    "print(platform_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc337c6",
   "metadata": {},
   "source": [
    "# 2. Preprocessing\n",
    "- remove NA content, duplicates, urls, from 'post_text' column.\n",
    "- use index str as `embed id` to avoid matching erros in long-id strings (with letters, numbers and special characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "249b84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# ===== PRE-COMPILE ALL REGEX PATTERNS (ONE TIME ONLY) =====\n",
    "URL_PATTERN = re.compile(r'https?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "PARTIAL_URL_PATTERN = re.compile(r'https?://\\S*')\n",
    "DOMAIN_PATTERN = re.compile(r'\\s(?:www\\.|(?:[\\w-]+\\.)+(?:com|net|org|edu|gov|mil|biz|info|io|me|tv|[\\w]{2,}))\\S*')\n",
    "FRAGMENTS_PATTERN = re.compile(r'(?:press\\.coop|gab\\.com|youtube\\.com|bitchute\\.com|imdb\\.com)\\/\\S*')\n",
    "ASCII_PATTERN = re.compile(r'[^\\x00-\\x7F]+')\n",
    "SPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "def remove_non_english(text):\n",
    "    \"\"\"\n",
    "    Language detection using langdetect\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) < 3:\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        detected_lang = detect(text)\n",
    "        if detected_lang != 'en':\n",
    "            return None\n",
    "        return text\n",
    "    except LangDetectException:\n",
    "        # If detection fails, keep the text\n",
    "        return text    \n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Remove all URLs from the text using pre-compiled patterns\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Use pre-compiled patterns (NO recompilation)\n",
    "    text = URL_PATTERN.sub('', text)\n",
    "    text = PARTIAL_URL_PATTERN.sub('', text)\n",
    "    text = DOMAIN_PATTERN.sub(' ', text)\n",
    "    text = FRAGMENTS_PATTERN.sub('', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function with optimized regex\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return None\n",
    "    \n",
    "    # First check if text is English (before removing content)\n",
    "    text = remove_non_english(text)\n",
    "    if text is None:\n",
    "        return None\n",
    "        \n",
    "    # Then remove URLs\n",
    "    text = remove_urls(text)\n",
    "    \n",
    "    # Remove emojis and non-ASCII characters using pre-compiled pattern\n",
    "    text = ASCII_PATTERN.sub('', text)\n",
    "    \n",
    "    # Remove multiple spaces using pre-compiled pattern\n",
    "    text = SPACE_PATTERN.sub(' ', text).strip()\n",
    "    \n",
    "    # Return None if text becomes empty after cleaning\n",
    "    return text if len(text) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "248915cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 86/555872 [00:00<10:48, 856.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 555872/555872 [14:42<00:00, 630.19it/s] \n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df = df.drop_duplicates(subset=['id'])\n",
    "df['post_clean'] = df['post_text'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53891a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data\n",
    "df = df.reset_index(drop=True)\n",
    "df['embed_id'] = df.index.astype(str)\n",
    "OUTPUT = \"/VData/scro4316/ct_prevalence/calvin_posts_cleaned.parquet\"\n",
    "df.to_parquet(OUTPUT, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e684ee",
   "metadata": {},
   "source": [
    "# 3. pre-embed texts with Mpnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74069fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/VData/scro4316/ct_prevalence/calvin_posts_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "149a4574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device: 1\n",
      "Using device: _CudaDeviceProperties(name='NVIDIA H100 80GB HBM3', major=9, minor=0, total_memory=81105MB, multi_processor_count=132, uuid=1cc4a5f1-9c3a-e533-51f5-2db38a687abb, L2_cache_size=50MB)\n",
      "Final validation: 554943 valid documents out of 555872\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import datetime\n",
    "import numpy as np\n",
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear any existing CUDA memory first\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Set GPU device to 1 (and verify it)\n",
    "torch.cuda.set_device(1)\n",
    "DEVICE = \"cuda:1\"\n",
    "print(\"Current CUDA device:\", torch.cuda.current_device())\n",
    "print(\"Using device:\", torch.cuda.get_device_properties(1))\n",
    "\n",
    "SERVER = \"/VData/scro4316/ct_prevalence\"\n",
    "OUTPUT_PATH = f\"{SERVER}/results\"\n",
    "EMBEDDINGS_OUTPUT_PATH = f\"{SERVER}/embeddings\"\n",
    "\n",
    "#  Embed claim text and save it to the embedding output ######\n",
    "# Prepare data for embedding (using unique texts only)\n",
    "ids = df['embed_id'].astype(str).tolist()  # Representative post_ids for unique texts\n",
    "docs = df['post_clean'].tolist()  # Unique texts only\n",
    "\n",
    "# Additional validation - ensure all docs are strings\n",
    "clean_docs = []\n",
    "clean_ids = []\n",
    "# ensure no 0-length strings after cleaning\n",
    "for i, doc in enumerate(docs):\n",
    "    if isinstance(doc, str) and len(doc.strip()) > 0:\n",
    "        clean_docs.append(doc.strip())\n",
    "        clean_ids.append(ids[i])\n",
    "\n",
    "print(f\"Final validation: {len(clean_docs)} valid documents out of {len(docs)}\")\n",
    "ids = clean_ids\n",
    "docs = clean_docs\n",
    "CHUNK_SIZE = 10000  # Number of documents per chunk\n",
    "\n",
    "#### 3. Generate embeddings and save them in chunks ######\n",
    "def generate_and_save_embeddings(docs, ids, output_path, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"\n",
    "    Generate embeddings for documents using OpenAI API and save them in chunks.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Verify GPU device before loading model\n",
    "    print(f\"Current GPU before model load: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    # Load embedding model\n",
    "    print(\"Loading all-mpnet-base-v2 model...\")\n",
    "    print(\"Before model load:\", torch.cuda.memory_allocated(device=DEVICE) / 1e9, \"GB\")\n",
    "    model = sentence_transformers.SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=DEVICE)\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(\"After model load:\", torch.cuda.memory_allocated(device=DEVICE) / 1e9, \"GB\")\n",
    "    \n",
    "    # Split into chunks\n",
    "    docs_chunks = [docs[x:x+chunk_size] for x in range(0, len(docs), chunk_size)]\n",
    "    ids_chunks = [ids[x:x+chunk_size] for x in range(0, len(ids), chunk_size)]\n",
    "    \n",
    "    for i in range(len(docs_chunks)):\n",
    "        out_file = f\"{output_path}/embeddings_{i+1}.npy\"\n",
    "        if os.path.isfile(out_file):\n",
    "            print(f\"Chunk {i+1} already exists, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing chunk {i+1} of {len(docs_chunks)}\")\n",
    "        print(f\"Starting at {datetime.datetime.now()}\")\n",
    "        print(f\"Current GPU device: {torch.cuda.current_device()}\")  # Verify GPU for each chunk\n",
    "        \n",
    "        # Generate embeddings for this chunk\n",
    "        embeddings = model.encode(docs_chunks[i], show_progress_bar=True, batch_size=32, device=DEVICE)\n",
    "        embeddings_dict = dict(zip(ids_chunks[i], embeddings))\n",
    "        \n",
    "        # Save chunk to file\n",
    "        np.save(out_file, embeddings_dict)\n",
    "        print(f\"Saved {len(embeddings_dict)} embeddings to {out_file}\")\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del embeddings\n",
    "        del embeddings_dict\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        print('Memory after cleanup:', round(torch.cuda.memory_allocated(1)/1024**3,1), 'GB')\n",
    "    \n",
    "    print(\"Embedding generation completed!\")\n",
    "\n",
    "def load_embeddings(file_path, ids_order=None):\n",
    "    \"\"\"Load embeddings from a numpy file.\"\"\"\n",
    "    try:\n",
    "        embeddings_dict = np.load(file_path, allow_pickle=True).item()\n",
    "        if ids_order:\n",
    "            embeddings_ordered = {id: embeddings_dict[id] for id in ids_order if id in embeddings_dict}\n",
    "            return embeddings_ordered\n",
    "        return embeddings_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load embeddings from {file_path} with error {e}\")\n",
    "        return None\n",
    "\n",
    "def load_all_embeddings(folder_path, ids_order=None):\n",
    "    \"\"\"Load all embeddings from a directory.\"\"\"\n",
    "    embeddings_dict = {}\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Embeddings folder {folder_path} does not exist!\")\n",
    "        return embeddings_dict\n",
    "    \n",
    "    # Get filenames and sort by number\n",
    "    filenames = sorted(os.listdir(folder_path), key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "    \n",
    "    for file_name in filenames:\n",
    "        if file_name.endswith('.npy'):\n",
    "            chunk_embeddings = load_embeddings(os.path.join(folder_path, file_name), ids_order)\n",
    "            if chunk_embeddings:\n",
    "                embeddings_dict.update(chunk_embeddings)\n",
    "                print(f\"Loaded {len(chunk_embeddings)} embeddings from {file_name}\")\n",
    "    \n",
    "    print(f\"Total embeddings loaded: {len(embeddings_dict)}\")\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_save_embeddings(docs, ids, EMBEDDINGS_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec888c",
   "metadata": {},
   "source": [
    "# 4. Bertopic modeling by topic and platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "299a6dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 embeddings from embeddings_1.npy\n",
      "Loaded 10000 embeddings from embeddings_2.npy\n",
      "Loaded 10000 embeddings from embeddings_3.npy\n",
      "Loaded 10000 embeddings from embeddings_4.npy\n",
      "Loaded 10000 embeddings from embeddings_5.npy\n",
      "Loaded 10000 embeddings from embeddings_6.npy\n",
      "Loaded 10000 embeddings from embeddings_7.npy\n",
      "Loaded 10000 embeddings from embeddings_8.npy\n",
      "Loaded 10000 embeddings from embeddings_9.npy\n",
      "Loaded 10000 embeddings from embeddings_10.npy\n",
      "Loaded 10000 embeddings from embeddings_11.npy\n",
      "Loaded 10000 embeddings from embeddings_12.npy\n",
      "Loaded 10000 embeddings from embeddings_13.npy\n",
      "Loaded 10000 embeddings from embeddings_14.npy\n",
      "Loaded 10000 embeddings from embeddings_15.npy\n",
      "Loaded 10000 embeddings from embeddings_16.npy\n",
      "Loaded 10000 embeddings from embeddings_17.npy\n",
      "Loaded 10000 embeddings from embeddings_18.npy\n",
      "Loaded 10000 embeddings from embeddings_19.npy\n",
      "Loaded 10000 embeddings from embeddings_20.npy\n",
      "Loaded 10000 embeddings from embeddings_21.npy\n",
      "Loaded 10000 embeddings from embeddings_22.npy\n",
      "Loaded 10000 embeddings from embeddings_23.npy\n",
      "Loaded 10000 embeddings from embeddings_24.npy\n",
      "Loaded 10000 embeddings from embeddings_25.npy\n",
      "Loaded 10000 embeddings from embeddings_26.npy\n",
      "Loaded 10000 embeddings from embeddings_27.npy\n",
      "Loaded 10000 embeddings from embeddings_28.npy\n",
      "Loaded 10000 embeddings from embeddings_29.npy\n",
      "Loaded 10000 embeddings from embeddings_30.npy\n",
      "Loaded 10000 embeddings from embeddings_31.npy\n",
      "Loaded 10000 embeddings from embeddings_32.npy\n",
      "Loaded 10000 embeddings from embeddings_33.npy\n",
      "Loaded 10000 embeddings from embeddings_34.npy\n",
      "Loaded 10000 embeddings from embeddings_35.npy\n",
      "Loaded 10000 embeddings from embeddings_36.npy\n",
      "Loaded 10000 embeddings from embeddings_37.npy\n",
      "Loaded 10000 embeddings from embeddings_38.npy\n",
      "Loaded 10000 embeddings from embeddings_39.npy\n",
      "Loaded 10000 embeddings from embeddings_40.npy\n",
      "Loaded 10000 embeddings from embeddings_41.npy\n",
      "Loaded 10000 embeddings from embeddings_42.npy\n",
      "Loaded 10000 embeddings from embeddings_43.npy\n",
      "Loaded 10000 embeddings from embeddings_44.npy\n",
      "Loaded 10000 embeddings from embeddings_45.npy\n",
      "Loaded 10000 embeddings from embeddings_46.npy\n",
      "Loaded 10000 embeddings from embeddings_47.npy\n",
      "Loaded 10000 embeddings from embeddings_48.npy\n",
      "Loaded 10000 embeddings from embeddings_49.npy\n",
      "Loaded 10000 embeddings from embeddings_50.npy\n",
      "Loaded 10000 embeddings from embeddings_51.npy\n",
      "Loaded 10000 embeddings from embeddings_52.npy\n",
      "Loaded 10000 embeddings from embeddings_53.npy\n",
      "Loaded 10000 embeddings from embeddings_54.npy\n",
      "Loaded 10000 embeddings from embeddings_55.npy\n",
      "Loaded 4943 embeddings from embeddings_56.npy\n",
      "Total embeddings loaded: 554943\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "embeddings = load_all_embeddings(EMBEDDINGS_OUTPUT_PATH, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from umap import UMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any\n",
    "def analyze_topic_platform(df: pd.DataFrame, \n",
    "                         topic: str, \n",
    "                         platform: str, \n",
    "                         embeddings_dict: Dict[str, np.ndarray],\n",
    "                         min_cluster_size: int = 20) -> tuple:\n",
    "    \"\"\"\n",
    "    Analyze documents for a specific topic and platform combination.\n",
    "    Returns topic model and document embeddings.\n",
    "    \"\"\"\n",
    "    # Filter data\n",
    "    mask = (df['topic'] == topic) & (df['platform'] == platform)\n",
    "    subset_df = df[mask].copy()\n",
    "    \n",
    "    # Print diagnostic information\n",
    "    print(f\"Found {len(subset_df)} documents for {topic} on {platform}\")\n",
    "    \n",
    "    # Filter for documents that have embeddings\n",
    "    subset_df = subset_df[subset_df['embed_id'].astype(str).isin(embeddings_dict.keys())].copy()\n",
    "    \n",
    "    if len(subset_df) < min_cluster_size:\n",
    "        print(f\"Insufficient data for {topic} on {platform}: {len(subset_df)} documents\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Get documents and their embeddings\n",
    "    doc_ids = subset_df['embed_id'].astype(str).tolist()\n",
    "    documents = subset_df['post_clean'].tolist()\n",
    "    \n",
    "    print(f\"Processing {len(documents)} documents after filtering\")\n",
    "    \n",
    "    try:\n",
    "        doc_embeddings = np.array([embeddings_dict[id_] for id_ in doc_ids])\n",
    "        \n",
    "        # Initialize models with more conservative parameters\n",
    "        umap_model = UMAP(\n",
    "            n_neighbors=min(15, len(documents)-1),  # Ensure n_neighbors is less than n_samples\n",
    "            n_components=2,\n",
    "            min_dist=0.1,\n",
    "            metric='cosine',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        vectorizer_model = CountVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=2  # At least 2 documents must contain the term\n",
    "        )\n",
    "        \n",
    "        # Initialize BERTopic with more robust parameters\n",
    "        topic_model = BERTopic(\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            min_topic_size=max(min_cluster_size, 5),  # Ensure minimum size is reasonable\n",
    "            nr_topics=\"auto\",\n",
    "            calculate_probabilities=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        topics, _ = topic_model.fit_transform(documents, doc_embeddings)\n",
    "        \n",
    "        # Check if any topics were found\n",
    "        if len(topic_model.get_topic_info()) <= 1:  # Only -1 topic means no clusters found\n",
    "            print(f\"No meaningful topics found for {topic} on {platform}\")\n",
    "            return None, None, None\n",
    "            \n",
    "        # Reduce embeddings for visualization\n",
    "        reduced_embeddings = umap_model.fit_transform(doc_embeddings)\n",
    "        \n",
    "        return topic_model, documents, reduced_embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {topic} on {platform}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "def visualize_results(topic_model: BERTopic, \n",
    "                     documents: List[str], \n",
    "                     reduced_embeddings: np.ndarray,\n",
    "                     topic: str,\n",
    "                     platform: str) -> None:\n",
    "    \"\"\"\n",
    "    Create and save visualizations for topic modeling results.\n",
    "    \"\"\"\n",
    "    # Get topic info\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    top_topics = topic_info[topic_info['Topic'] != -1].head(10)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot each topic\n",
    "    unique_topics = top_topics['Topic'].tolist()\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_topics)))\n",
    "    \n",
    "    # Plot points\n",
    "    for idx, t in enumerate(unique_topics):\n",
    "        mask = np.array(topic_model.topics_) == t\n",
    "        if np.any(mask):\n",
    "            points = reduced_embeddings[mask]\n",
    "            # Get top 10 words for this topic\n",
    "            words = topic_model.get_topic(t)[:10]\n",
    "            label = '_'.join([word[0] for word in words])\n",
    "            plt.scatter(points[:, 0], points[:, 1], \n",
    "                       c=[colors[idx]], \n",
    "                       label=f\"Topic {t}: {label}\",\n",
    "                       alpha=0.6, s=20)\n",
    "    \n",
    "    plt.title(f'Topic Distribution: {topic} on {platform}')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f'/VData/scro4316/ct_prevalence/results/topic_viz_{topic}_{platform}.png', \n",
    "                bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7cdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for each topic-platform combination...\n",
      "\n",
      "Analyzing KEYWORDS_COVID19 on X\n",
      "==================================================\n",
      "Found 34998 documents for KEYWORDS_COVID19 on X\n",
      "Processing 34900 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:14:22,916 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-03 12:14:50,162 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 12:14:50,165 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 12:15:59,790 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 12:15:59,793 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 12:16:02,326 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 12:16:02,329 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 12:16:02,408 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 12:16:04,675 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 12:16:04,678 - BERTopic - Topic reduction - Reduced number of topics from 220 to 4\n",
      "2025-11-03 12:16:34,927 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_COVID19 on X:\n",
      "Total documents: 34900\n",
      "\n",
      "Top 10 topics:\n",
      "   Topic  Count                                           Name\n",
      "1      0  18817               0_covid_vaccine_vaccines_covid19\n",
      "2      1     96                                 1_la_il_di_che\n",
      "3      2     25  2_english_golden_vaccine created_like covid19\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_COVID19 on fediverse\n",
      "==================================================\n",
      "Found 6096 documents for KEYWORDS_COVID19 on fediverse\n",
      "Processing 6082 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:16:43,169 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 12:16:43,170 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 12:16:43,873 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 12:16:43,874 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 12:16:45,250 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 12:16:45,252 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 12:16:45,266 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 12:16:46,491 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 12:16:46,494 - BERTopic - Topic reduction - Reduced number of topics from 64 to 28\n",
      "2025-11-03 12:16:55,017 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_COVID19 on fediverse:\n",
      "Total documents: 6082\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                         Name\n",
      "1       0   1949             0_covid_vaccine_vaccines_covid19\n",
      "2       1    125                    1_woke_patriot_says_trump\n",
      "3       2    124                      2_jews_lie_russians_jew\n",
      "4       3    115                  3_usaid_funded_musk_funding\n",
      "5       4     66  4_scamdemic_cdc grooming_grooming_new world\n",
      "6       5     65               5_economy_assets_wealth_market\n",
      "7       6     56        6_canada_canadian_minister_government\n",
      "8       7     52          7_ivermectin_cancer_drug_remdesivir\n",
      "9       8     49        8_covid_memes_covid1984_covid covid19\n",
      "10      9     48       9_cancer_cancers_aggressive_soonshiong\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_COVID19 on bluesky\n",
      "==================================================\n",
      "Found 27487 documents for KEYWORDS_COVID19 on bluesky\n",
      "Processing 27467 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:17:14,835 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 12:17:14,836 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 12:17:27,545 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 12:17:27,546 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 12:17:29,205 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 12:17:29,206 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 12:17:29,236 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 12:17:30,849 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 12:17:30,852 - BERTopic - Topic reduction - Reduced number of topics from 122 to 55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_COVID19 on bluesky:\n",
      "Total documents: 27467\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                          Name\n",
      "1       0  11054               0_covid_people_vaccines_vaccine\n",
      "2       1    384                     1_masks_mask_wear_wearing\n",
      "3       2    276                     2_uk_tories_labour_brexit\n",
      "4       3    176                         3_shes_covid_lady_did\n",
      "5       4    172        4_fauci_anthony_anthony fauci_function\n",
      "6       5    163                  5_musk_elon_trump_trump musk\n",
      "7       6    116               6_canada_trudeau_ford_canadians\n",
      "8       7    104  7_facebook_zuckerberg_censor_mark zuckerberg\n",
      "9       8    103  8_plandemic_real plandemic_real_plandemic 20\n",
      "10      9     98                  9_ballots_2020_election_mail\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_COVID19 on truthsocial\n",
      "==================================================\n",
      "Found 98878 documents for KEYWORDS_COVID19 on truthsocial\n",
      "Processing 98799 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:17:53,926 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-03 12:21:28,204 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 12:21:28,208 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 16:53:03,482 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 16:53:03,496 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 16:53:16,055 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 16:53:16,060 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 16:53:17,338 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 16:53:28,543 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 16:53:28,556 - BERTopic - Topic reduction - Reduced number of topics from 1017 to 811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_COVID19 on truthsocial:\n",
      "Total documents: 98799\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                                                 Name\n",
      "1       0   1583                                             0_china_chinese_ccp_land\n",
      "2       1   1414  1_involved_collusion hoax_involved russia_involved russia collusion\n",
      "3       2   1243                                          2_hes_heaposs_needs_nursing\n",
      "4       3   1114                                      3_mrna_mortality_studies_excess\n",
      "5       4   1024                                                 4_gop_zero_math_dems\n",
      "6       5    965                                    5_fauci_dr fauci_aids_fauci needs\n",
      "7       6    952                                                  6_flu_came_lab_died\n",
      "8       7    935                                         7_bird_bird flu_chickens_flu\n",
      "9       8    822               8_zuckerberg_ad_rockefeller foundation_mark zuckerberg\n",
      "10      9    747                                              9_shot_got_took_vaccine\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_COVID19 on 4chan\n",
      "==================================================\n",
      "Found 16369 documents for KEYWORDS_COVID19 on 4chan\n",
      "Processing 16358 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 16:55:40,476 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-03 16:55:51,408 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 16:55:51,411 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 16:55:53,495 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 16:55:53,496 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 16:55:55,733 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 16:55:55,735 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 16:55:55,747 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 16:55:57,854 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 16:55:57,858 - BERTopic - Topic reduction - Reduced number of topics from 61 to 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_COVID19 on 4chan:\n",
      "Total documents: 16358\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                 Name\n",
      "1       0   5273          0_covid_vaccine_people_just\n",
      "2       1    627              1_jews_jewish_jew_covid\n",
      "3       2    523       2_clot_clot shot_shot_clotshot\n",
      "4       3    232           3_ukraine_russia_war_putin\n",
      "5       4    223              4_people_shit_like_just\n",
      "6       5    201             5_eggs_chickens_egg_bird\n",
      "7       6    175  6_canada_canadians_trudeau_canadian\n",
      "8       7    154             7_women_white_men_people\n",
      "9       8    128              8_god_beast_mark_christ\n",
      "10      9    113                  9_5g_cloud_ngo_able\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_COVID19 on gettr\n",
      "==================================================\n",
      "Found 28834 documents for KEYWORDS_COVID19 on gettr\n",
      "Processing 28762 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 16:56:09,054 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-03 16:56:31,625 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 16:56:31,627 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-03 17:00:24,049 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-03 17:00:24,052 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-11-03 17:00:28,070 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 17:00:28,075 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-03 17:00:28,283 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-03 17:00:32,258 - BERTopic - Representation - Completed ✓\n",
      "2025-11-03 17:00:32,262 - BERTopic - Topic reduction - Reduced number of topics from 377 to 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for KEYWORDS_COVID19 on gettr:\n",
      "Total documents: 28762\n",
      "\n",
      "Top 10 topics:\n",
      "    Topic  Count                                           Name\n",
      "1       0  17830               0_covid_vaccine_covid19_vaccines\n",
      "2       1    105                       1_jews_israel_jewish_jew\n",
      "3       2     95    2_plandemic_plan_plandemic 20_new plandemic\n",
      "4       3     74                   3_newsom_california_la_gavin\n",
      "5       4     74                 4_dangerous_uk_government_just\n",
      "6       5     65              5_birx_covid_deborah birx_deborah\n",
      "7       6     60              6_carney_canada_trudeau_canadians\n",
      "8       7     57       7_prosecuted_fully_willful_fully exposed\n",
      "9       8     55  8_davos_party_davos party_virus covid vaccine\n",
      "10      9     50                 9_pope_church_vatican_catholic\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analyzing KEYWORDS_COVID19 on gab\n",
      "==================================================\n",
      "Found 62795 documents for KEYWORDS_COVID19 on gab\n",
      "Processing 62634 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 17:00:53,074 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-03 17:01:42,579 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-03 17:01:42,582 - BERTopic - Cluster - Start clustering the reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "# Process each topic-platform combination\n",
    "# topics = df['topic'].unique().tolist()\n",
    "# topics = ['KEYWORDS_9_11','KEYWORDS_ALIEN', 'KEYWORDS_MOON']\n",
    "topics = ['KEYWORDS_COVID19','KEYWORDS_NWO']\n",
    "\n",
    "platforms = df['platform'].unique().tolist()\n",
    "print(\"Starting analysis for each topic-platform combination...\")\n",
    "results_summary = []\n",
    "for topic in topics:\n",
    "    for platform in platforms:\n",
    "        print(f\"\\nAnalyzing {topic} on {platform}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            topic_model, documents, reduced_embeddings = analyze_topic_platform(\n",
    "                df, topic, platform, embeddings\n",
    "            )\n",
    "            \n",
    "            if topic_model is not None and documents is not None:\n",
    "                # Get and display topic information\n",
    "                topic_info = topic_model.get_topic_info()\n",
    "                if len(topic_info) > 1:  # More than just the -1 topic\n",
    "                    top_topics = topic_info[topic_info['Topic'] != -1].head(10)\n",
    "                    \n",
    "                    # Store results\n",
    "                    results_summary.append({\n",
    "                        'topic': topic,\n",
    "                        'platform': platform,\n",
    "                        'n_documents': len(documents),\n",
    "                        'top_topics': top_topics\n",
    "                    })\n",
    "                    \n",
    "                    # Create visualization\n",
    "                    visualize_results(topic_model, documents, reduced_embeddings, topic, platform)\n",
    "                    \n",
    "                    # Print summary\n",
    "                    print(f\"\\nResults for {topic} on {platform}:\")\n",
    "                    print(f\"Total documents: {len(documents)}\")\n",
    "                    print(\"\\nTop 10 topics:\")\n",
    "                    print(top_topics[['Topic', 'Count', 'Name']].to_string())\n",
    "                else:\n",
    "                    print(f\"No meaningful topics found for {topic} on {platform}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {topic} on {platform}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b55214fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "Topic: KEYWORDS_COVID19\n",
      "----------------------------------------\n",
      "              X fediverse bluesky truthsocial 4chan gettr   gab\n",
      "Topic_Num                                                      \n",
      "Topic 0    null      null    null        null  null  null  null\n",
      "Topic 1    null      null    null        null  null  null  null\n",
      "Topic 2    null      null    null        null  null  null  null\n",
      "Topic 3    null      null    null        null  null  null  null\n",
      "Topic 4    null      null    null        null  null  null  null\n",
      "Topic 5    null      null    null        null  null  null  null\n",
      "Topic 6    null      null    null        null  null  null  null\n",
      "Topic 7    null      null    null        null  null  null  null\n",
      "Topic 8    null      null    null        null  null  null  null\n",
      "Topic 9    null      null    null        null  null  null  null\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Topic: KEYWORDS_NWO\n",
      "----------------------------------------\n",
      "              X fediverse bluesky truthsocial 4chan gettr   gab\n",
      "Topic_Num                                                      \n",
      "Topic 0    null      null    null        null  null  null  null\n",
      "Topic 1    null      null    null        null  null  null  null\n",
      "Topic 2    null      null    null        null  null  null  null\n",
      "Topic 3    null      null    null        null  null  null  null\n",
      "Topic 4    null      null    null        null  null  null  null\n",
      "Topic 5    null      null    null        null  null  null  null\n",
      "Topic 6    null      null    null        null  null  null  null\n",
      "Topic 7    null      null    null        null  null  null  null\n",
      "Topic 8    null      null    null        null  null  null  null\n",
      "Topic 9    null      null    null        null  null  null  null\n",
      "\n",
      "\n",
      "Results have been saved to: /VData/scro4316/ct_prevalence/results\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "results_dir = Path(\"/VData/scro4316/ct_prevalence/results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Store results for each topic-platform combination\n",
    "topic_platform_results = {}\n",
    "\n",
    "# First, save individual results as CSVs\n",
    "for result in results_summary:\n",
    "    topic = result['topic']\n",
    "    platform = result['platform']\n",
    "    top_topics_df = result['top_topics']\n",
    "    \n",
    "    # Add representative words column\n",
    "    top_topics_df['Rep_Words'] = top_topics_df['Name'].apply(lambda x: ', '.join(x.split('_')[:10]))\n",
    "    top_topics_df['Doc_Count'] = top_topics_df['Count']\n",
    "    \n",
    "    # Save to CSV\n",
    "    filename = f\"{topic}_{platform}_topics.csv\"\n",
    "    top_topics_df.to_csv(results_dir / filename)\n",
    "    \n",
    "    # Store in dictionary for table creation\n",
    "    if topic not in topic_platform_results:\n",
    "        topic_platform_results[topic] = {}\n",
    "    topic_platform_results[topic][platform] = {\n",
    "        'topics': top_topics_df['Rep_Words'].tolist(),\n",
    "        'counts': top_topics_df['Count'].tolist()\n",
    "    }\n",
    "\n",
    "topics = ['KEYWORDS_COVID19','KEYWORDS_NWO']\n",
    "# Create comparison tables for each topic\n",
    "for topic in topics:\n",
    "    print(f\"\\n{'-'*40}\")\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(f\"{'-'*40}\")\n",
    "    \n",
    "    # Create DataFrame with platforms as columns\n",
    "    table_data = []\n",
    "    for i in range(10):  # For each of the top 10 topics\n",
    "        row = {'Topic_Num': f'Topic {i}'}\n",
    "        for platform in platforms:\n",
    "            if platform in topic_platform_results.get(topic, {}):\n",
    "                platform_data = topic_platform_results[topic][platform]\n",
    "                if i < len(platform_data['topics']):\n",
    "                    words = platform_data['topics'][i]\n",
    "                    count = platform_data['counts'][i]\n",
    "                    row[platform] = f\"[{words}], n={count}\"\n",
    "                else:\n",
    "                    row[platform] = \"null\"\n",
    "            else:\n",
    "                row[platform] = \"null\"\n",
    "        table_data.append(row)\n",
    "    \n",
    "    # Create and display the table\n",
    "    comparison_df = pd.DataFrame(table_data)\n",
    "    comparison_df.set_index('Topic_Num', inplace=True)\n",
    "    \n",
    "    # Save comparison table\n",
    "    filename = f\"{topic}_platform_comparison.csv\"\n",
    "    comparison_df.to_csv(results_dir / filename)\n",
    "    \n",
    "    # Display the table\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(comparison_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"Results have been saved to:\", results_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ctplatform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
